{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Last class\n",
    "\n",
    "On Thursday, September 28, I talked about how a broad class of machine learning fits into a simple plan.  Then, I talked a bit about linear algebra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Machine Learning Plan\n",
    "<ol>\n",
    "    <li>Turn the problem you want to solve into a function $f:\\mathbb{R}^N \\to \\mathbb{R}$</li>\n",
    "    <li>Identify a class of functions, say $\\{p_A\\}$ depending on a parameter $A=\\{a_0, \\ldots, a_d\\}$ from a subset of $\\mathbb{R}^d$ from which you will try to find a best approximation to $f$.</li>\n",
    "    <li>Create a cost function $C:\\mathbb{R}^d \\to \\mathbb{R}$ that measures the error between $f$ and $p_A$.  For example, if $\\{v_1, \\ldots, v_M\\}$ are points in $\\mathbb{R}^N$ consisting of some training data with known values $f(v_i)$, a reasonable cost function might be something like\n",
    "    $$C(A)=\\sum_{i=1}^M (f(v_i)-p_A(v_i))^2.$$</li>\n",
    "    <li>Finally, find the parameters $A$ that minimize the cost.  If everything depends smoothly on the parameters, then often some kind of gradient descent can be used.  If all goes well, the resulting function $p_A$ is a good model for the function $f$ and can be used to approximate $f$ on new data.</li>\n",
    "    </ol>\n",
    "\n",
    "You're last assignment was precisely to solve a problem using this plan.  Your class of functions $\\{p_a\\}$ were polynomials, but one could choose different functions.  For example, you might have reason to choose combinations of exponentials such as $a_0 \\exp( a_1 x)+ a_2 \\exp(a_3 x)$, or Fourier polynomials. Neural networks is the name of a particular class of functions used for approximations that are particularly popular now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Linear algebra\n",
    "Inner product spaces, which are often, but not always, covered in a first linear algebra course, are important in many branches of mathematics and it behooves you to know them well.  In this class, I plan to use inner product spaces in at least two ways in the next few weeks:\n",
    "<ul>\n",
    "    <li>To describe a way to effeciently minimize certain cost functions without using gradient descent or derivatives at all.</li>\n",
    "    <li>To begin to describe enhanced versions of gradient descent, like conjugate gradient descent.</li>\n",
    "</ul>\n",
    "For a basic review of inner product spaces, I recommend Sheldon Axler's <a href=\"http://linear.axler.net/index.html\"><em>Linear Algebra Done Right</em></a>. I think the entire book is excellent and recommend it highly.  You'll want to know that there's a sample chapter on Inner Product Spaces available <a href=\"http://linear.axler.net/InnerProduct.pdf\">http://linear.axler.net/InnerProduct.pdf</a> and an abridged version of the entire book is available for free <a href=\"http://linear.axler.net/LinearAbridged.pdf\">http://linear.axler.net/LinearAbridged.pdf</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Things to study this weekend\n",
    "<ul>\n",
    "    <li>Keep learning python.  You've got R.J. Johansson's <a href=\"https://github.com/jrjohansson/scientific-python-lectures\">scientific computing with python</a> notebooks to work through.</li>\n",
    "    <li>Review inner product spaces, in particular the Gram-Schmidt process for finding an orthonormal basis.  Write a program to test your understanding of this process.  A first step would be to write a program with a list of vectors in say R^5 as input that outputs an orthonormal list obtained from the input by Gram-Schmidt process.  After that, you might want to generalize to $\\mathbb{R}^n$, or to an arbitrary inner product space.</li>\n",
    "    <li>Experiment with the versions of gradient descent that we worked on at the beginning of last week to improve the speed of the algorithm.</li>\n",
    "    <li>Try to work out some examples on your own that illustrate the problem of <a href=\"https://en.wikipedia.org/wiki/Overfitting\"><em>overfitting</em></a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
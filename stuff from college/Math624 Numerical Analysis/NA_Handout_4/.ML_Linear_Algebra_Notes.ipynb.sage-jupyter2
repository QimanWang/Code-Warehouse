{"type":"settings","kernel":"anaconda3","backend_state":"running","metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"}},"kernel_usage":{"cpu":0,"memory":88297472},"kernel_state":"idle"}
{"type":"cell","id":"0ea713","pos":0,"input":"# Last class\n\nOn Thursday, September 28, I talked about how a broad class of machine learning fits into a simple plan.  Then, I talked a bit about linear algebra.\n","cell_type":"markdown"}
{"type":"cell","id":"fbd15c","pos":1,"input":"## Machine Learning Plan\n<ol>\n    <li>Turn the problem you want to solve into a function $f:\\mathbb{R}^N \\to \\mathbb{R}$</li>\n    <li>Identify a class of functions, say $\\{p_A\\}$ depending on a parameter $A=\\{a_0, \\ldots, a_d\\}$ from a subset of $\\mathbb{R}^d$ from which you will try to find a best approximation to $f$.</li>\n    <li>Create a cost function $C:\\mathbb{R}^d \\to \\mathbb{R}$ that measures the error between $f$ and $p_A$.  For example, if $\\{v_1, \\ldots, v_M\\}$ are points in $\\mathbb{R}^N$ consisting of some training data with known values $f(v_i)$, a reasonable cost function might be something like\n    $$C(A)=\\sum_{i=1}^M (f(v_i)-p_A(v_i))^2.$$</li>\n    <li>Finally, find the parameters $A$ that minimize the cost.  If everything depends smoothly on the parameters, then often some kind of gradient descent can be used.  If all goes well, the resulting function $p_A$ is a good model for the function $f$ and can be used to approximate $f$ on new data.</li>\n    </ol>\n\nYou're last assignment was precisely to solve a problem using this plan.  Your class of functions $\\{p_a\\}$ were polynomials, but one could choose different functions.  For example, you might have reason to choose combinations of exponentials such as $a_0 \\exp( a_1 x)+ a_2 \\exp(a_3 x)$, or Fourier polynomials. Neural networks is the name of a particular class of functions used for approximations that are particularly popular now.","cell_type":"markdown"}
{"type":"cell","id":"9c91c7","pos":2,"input":"","cell_type":"code","exec_count":0}
{"type":"cell","id":"ec40dc","pos":3,"input":"## Linear algebra\nInner product spaces, which are often, but not always, covered in a first linear algebra course, are important in many branches of mathematics and it behooves you to know them well.  In this class, I plan to use inner product spaces in at least two ways in the next few weeks:\n<ul>\n    <li>To describe a way to effeciently minimize certain cost functions without using gradient descent or derivatives at all.</li>\n    <li>To begin to describe enhanced versions of gradient descent, like conjugate gradient descent.</li>\n</ul>\nFor a basic review of inner product spaces, I recommend Sheldon Axler's <a href=\"http://linear.axler.net/index.html\"><em>Linear Algebra Done Right</em></a>. I think the entire book is excellent and recommend it highly.  You'll want to know that there's a sample chapter on Inner Product Spaces available <a href=\"http://linear.axler.net/InnerProduct.pdf\">http://linear.axler.net/InnerProduct.pdf</a> and an abridged version of the entire book is available for free <a href=\"http://linear.axler.net/LinearAbridged.pdf\">http://linear.axler.net/LinearAbridged.pdf</a>.\n","cell_type":"markdown"}
{"type":"cell","id":"fc0a0a","pos":4,"input":"## Things to study this weekend\n<ul>\n    <li>Keep learning python.  You've got R.J. Johansson's <a href=\"https://github.com/jrjohansson/scientific-python-lectures\">scientific computing with python</a> notebooks to work through.</li>\n    <li>Review inner product spaces, in particular the Gram-Schmidt process for finding an orthonormal basis.  Write a program to test your understanding of this process.  A first step would be to write a program with a list of vectors in say R^5 as input that outputs an orthonormal list obtained from the input by Gram-Schmidt process.  After that, you might want to generalize to $\\mathbb{R}^n$, or to an arbitrary inner product space.</li>\n    <li>Experiment with the versions of gradient descent that we worked on at the beginning of last week to improve the speed of the algorithm.</li>\n    <li>Try to work out some examples on your own that illustrate the problem of <a href=\"https://en.wikipedia.org/wiki/Overfitting\"><em>overfitting</em></a></li>","cell_type":"markdown"}
{"type":"cell","id":"3ed4aa","pos":5,"input":"","cell_type":"code","exec_count":0}
{"type":"file","last_load":1506972030881}
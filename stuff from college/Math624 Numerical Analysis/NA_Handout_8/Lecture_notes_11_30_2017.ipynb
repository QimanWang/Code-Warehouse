{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tips about lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip is a useful function to merge and unmerge two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3] ; y = [4, 5, 6]\n",
    "z = zip(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use zip with * to unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2 is (1, 2, 3)\n",
      "y2 is (4, 5, 6)\n"
     ]
    }
   ],
   "source": [
    "[x2, y2] = zip(*z)\n",
    "print \"x2 is\", x2\n",
    "print \"y2 is\", y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### you can construct lists using something like set builder notation\n",
    "\n",
    "In math, we easily understand a statement like this\n",
    "\n",
    "$$\\left\\{2x_i+y_i \\;\\vert\\; i=0, \\ldots, 3\\right\\}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mylist = [[1,2],[2,5],[5,4],[4,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "9\n",
      "14\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "for x,y in mylist:\n",
    "    print 2*x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.   9.  14.  14.]\n"
     ]
    }
   ],
   "source": [
    "newlist=np.zeros(len(mylist))\n",
    "counter=0\n",
    "for x,y in mylist:\n",
    "    newlist[counter]=2*x+y\n",
    "    counter+=1\n",
    "print newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  9 14 14]\n"
     ]
    }
   ],
   "source": [
    "newlist2 = np.array([2*x+y for x,y in mylist])\n",
    "print newlist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a network and computing the gradient\n",
    "\n",
    "We'll need to define the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep our parameters organized, it will help to have a picture.  Suppose, for example, somewhere in our net we have a layer of three nodes connected to a layer of four nodes.  Then we will have $12$ weights on the edges between these layers.  Denote the weight connecting the $k$-th node from the layer on the left to the $j$-th node of the layer on the right by $w_{jk}$.  For example, in the picture, $w_{21}=1.2$, $w_{22}=0.7$, and $w_{23}=-0.4$.  The other parameters are the biases $b_1, b_2, b_3, b_4$.  In the picture $b_2=2.1$.\n",
    "\n",
    "<img src=\"Figure.png\">  \n",
    "\n",
    "The output of the nodes on the left are numbers, labelled here by $(z_1, z_2, z_3)$.  Then the output of the nodes in the next layer are determined by weights and the biases.  In the picture, the output of the second node on the right is \n",
    "\n",
    "$$\\sigma \\left( \\begin{bmatrix} 1.2 & 0.7 & -0.4\\end{bmatrix} \\begin{bmatrix} z_{1} \\\\ z_{2}\\\\ z_{3}\\end{bmatrix} + 2.1\\right)$$\n",
    "\n",
    "More generally, the output of all the nodes on the right are given by \n",
    "\n",
    "$$\\sigma \\left( \\begin{bmatrix} w_{11} & w_{12}& w_{13}\\\\\n",
    "w_{21} & w_{22}& w_{23}\\\\w_{31} & w_{32}& w_{33}\\\\\n",
    "w_{41} & w_{42}& w_{43}\\end{bmatrix} \\begin{bmatrix} z_{1} \\\\ z_{2}\\\\ z_{3}\\end{bmatrix} + \\begin{bmatrix}  b_1 \\\\ b_2 \\\\b_3\\\\b_4 \\end{bmatrix}\\right)$$\n",
    "\n",
    "where we use the convention that $\\sigma$ applied to a vector means $\\sigma$ applied to each element.  \n",
    "\n",
    "Finally, keep in mind this picture is just part of our net, say connecting the $i$-th layer to the $i+1$-st layey.  To make this explicit, we'll decorate everything with a superscript $i$ to keep track of which layer we're in.  So, we let\n",
    "\n",
    "<ul>\n",
    "<li>$W=\\{w^i_{jk}$ be the matrix of weights connecting the $k$-th node of the $i$-th layer to the $j$-th node of the $i+1$-st layer</li>\n",
    "<li>$z^i$ be the vector of outputs of the $i$-th layer</li>\n",
    "<li>$B^i$ be the vector of biases that we input in the $i+1$-st layer</li>\n",
    "</ul>\n",
    "\n",
    "In this notation, the output of the $i+1$-st layer is\n",
    "\n",
    "$$z^{i+1}=\\sigma \\left( W^iz^i+B^i\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mysizes = [3,5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeinitial(sizes):\n",
    "    biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "    weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    return [biases,weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mybiases,myweights=makeinitial(mysizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finaloutput(biases,weights, x):\n",
    "    for b, w in zip(biases, weights):\n",
    "        x = sigmoid(np.dot(w, x)+b)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(biases,weights,x,y):\n",
    "    error = finaloutput(biases,weights,x)-y\n",
    "    return 0.5*np.dot(error.transpose(),error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.76095814]\n",
      " [ 0.10334163]\n",
      " [ 1.16587609]]\n"
     ]
    }
   ],
   "source": [
    "myx=np.random.randn(3,1);\n",
    "print myx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23534959],\n",
       "       [ 0.17733111]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaloutput(mybiases,myweights,myx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myy=np.random.rand(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44992961]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(mybiases,myweights,myx,myy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alloutputs(biases,weights, x):\n",
    "        layers=[]\n",
    "        layer=x\n",
    "        layers.append(layer)\n",
    "        for b, w in zip(biases,weights):\n",
    "            z = np.dot(w,layer)+b\n",
    "            #print \"input\", layer\n",
    "            #print \"weight\", w\n",
    "            #print \"biases\", b\n",
    "            #print \"z\", z\n",
    "            layer=sigmoid(z)\n",
    "            layers.append(layer)\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.76095814],\n",
       "        [ 0.10334163],\n",
       "        [ 1.16587609]]), array([[ 0.58959644],\n",
       "        [ 0.06096265],\n",
       "        [ 0.31746618],\n",
       "        [ 0.81024644],\n",
       "        [ 0.21492882]]), array([[ 0.23534959],\n",
       "        [ 0.17733111]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alloutputs(mybiases,myweights,myx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dcost(biases,weights, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "        num_layers = len(biases)+1\n",
    "        layers=[]\n",
    "        layer=x\n",
    "        layers.append(layer)\n",
    "        for b, w in zip(biases,weights):\n",
    "            z = np.dot(w,layer)+b\n",
    "            layer=sigmoid(z)\n",
    "            layers.append(layer)\n",
    "        delta = (layers[-1]-y)*layers[-1]*(1-layers[-1])\n",
    "        # print delta\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, layers[-2].transpose())\n",
    "        for l in range(2, num_layers):\n",
    "            z = layers[-l]\n",
    "            ds = z*(1-z)\n",
    "            delta = np.dot(weights[-l+1].transpose(), delta) * ds\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, (layers[-l-1]).transpose())\n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbiases,dweights=dcost(mybiases,myweights,myx,myy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mybiases2,myweights2=makeinitial(mysizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mybiases2=[np.copy(mybiases[0]), np.copy(mybiases[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mybiases2[0][1]=mybiases2[0][1]+.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00393743]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cost(mybiases2,myweights,myx,myy)-cost(mybiases,myweights,myx,myy))*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00393741])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbiases[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Neilsen's book <a href=\"http://neuralnetworksanddeeplearning.com\">Neural Networks\n",
    "and Deep Learning\"</a>, we can define a general network class to store the parameters for our neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"sizes is a list of the number of neurons in each layer.  \n",
    "        The weights and biases are initialized randomly.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    def finaloutput(self, x):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            x = sigmoid(np.dot(w, x)+b)\n",
    "        return x\n",
    "    \n",
    "    def alloutputs(self, x):\n",
    "        layers=[]\n",
    "        layer=x\n",
    "        layers.append(layer)\n",
    "        for b, w in zip(self.biases,self.weights):\n",
    "            z = np.dot(w,layer)+b\n",
    "            layer=sigmoid(z)\n",
    "            layers.append(layer)\n",
    "        return layers\n",
    "    \n",
    "    def dcost(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        layers=[]\n",
    "        layer=x\n",
    "        layers.append(layer)\n",
    "        for b, w in zip(self.biases,self.weights):\n",
    "            z = np.dot(w,layer)+b\n",
    "            layer=sigmoid(z)\n",
    "            layers.append(layer)\n",
    "        delta = (layers[-1]-y)*layers[-1]*(1-layers[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, layers[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = layers[-l]\n",
    "            ds = z*(1-z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * ds\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, (layers[-l-1]).transpose())\n",
    "        return (nabla_b, nabla_w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

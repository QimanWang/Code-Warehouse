{"type":"settings","kernel":"anaconda3","backend_state":"running","trust":true,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"}},"kernel_usage":{"cpu":0,"memory":87228416},"kernel_state":"idle"}
{"type":"file","last_load":1508197128356}
{"type":"cell","id":"823b6e","pos":19,"input":"'scatter' plots the points as dots","cell_type":"markdown"}
{"type":"cell","id":"461d6c","pos":28,"input":"fig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7]) # left, bottom, width, height (range 0 to 1)\naxes.plot(x,y, 'g', label=r'$y = x^4-3x^3+2$') # g for green\naxes.scatter(x_list,y_list,c=\"r\")\naxes.plot(x_list,y_list,c=\"r\",label='gradient descent steps')\naxes.legend(loc=2); # upper left corner\naxes.set_xlabel('$x$')\naxes.set_ylabel('$y$') ;","cell_type":"code","exec_count":0}
{"type":"cell","id":"b6476d","pos":31,"input":"## 2 dimensional example","cell_type":"markdown"}
{"type":"cell","id":"5b3252","pos":15,"input":"f(2)","output":{"0":{"data":{"text/plain":"-6"},"output_type":"execute_result","exec_count":5}},"cell_type":"code","exec_count":5}
{"type":"cell","id":"b9c7d9","pos":56,"input":"p5","cell_type":"code","exec_count":0}
{"type":"cell","id":"5843d4","pos":5,"input":"### Steepest Descent\n\nRecall that $\\langle v,w \\rangle=\\|v\\|w\\| \\cos(\\theta)$ where $\\theta$ is the angle between the vectors $v$ and $w$.  Therefore, if $v$ is a unit vector, \n\n$$D_v g(x)=\\langle \\nabla g(x), v\\rangle =\\|\\nabla g(x)\\| \\cos(\\theta)$$\n\nis maximized when $v$ points in the direction of the gradient $\\nabla g(x)$ and minimized when $v$ points in the direction $-\\nabla g(x)$.\n\n<strong>Conclusion</strong> $-\\nabla g(x)$ is the direction of steepest descent for the function $g$ at the point $x$.","cell_type":"markdown"}
{"type":"cell","id":"c8d6fd","pos":33,"input":"def g(x):\n    return 3*(x[0]-2)**2+(x[1]-1)**2+5","cell_type":"code","exec_count":0}
{"type":"cell","id":"cb332c","pos":16,"input":"To plot this function, we make a bunch of (x,y) coordinates on the graph.  Before doing this, let's take a look at ways we can operate on elements in a list.","cell_type":"markdown"}
{"type":"cell","id":"77bdc1","pos":38,"input":"plt.figure()\nplt.contour(X0,X1,Z,50) # this plots level sets\nplt.plot(x0_coordlist,x1_coordlist,c=\"r\"); # here is the gradient path","cell_type":"code","exec_count":0}
{"type":"cell","id":"e9138a","pos":6,"input":"### Perpendicular to level curves\n\nThe level set of $g:\\mathbb{R}^n \\to \\mathbb{R}$ of level $k\\in \\mathbb{R}$ is defined to be the set\n\n$$g^{-1}(k)=\\{(x_0,x_1,  \\ldots, x_{n-1})\\in \\mathbb{R}^n: g(x_0,x_1,  \\ldots, x_{n-1})=k\\}$$\n\nIf $:\\alpha: t\\mapsto (x_0(t),x_1(t),  \\ldots, x_{n-1}(t))$ is a curve in a level set of $g$, then the the function $\\mathbb{R} \\overset{\\alpha}{\\to} \\mathbb{R}^n \\overset{g}{\\to} \\mathbb{R}$ is constant and hence has zero derivative.  Keeping the multivariable chain rule in mind, we have\n\n$$Dg \\circ d\\alpha = 0 \\Longleftrightarrow \\begin{bmatrix} \\frac{\\partial g}{\\partial x_0} (x)& \\frac{\\partial g}{\\partial x_1} (x)& \\ldots & \\frac{\\partial g}{\\partial x_{n-1}} (x) \\end{bmatrix} \\begin{bmatrix} x_0'(t) \\\\ x_1'(t) \\\\ \\vdots \\\\ x_{n-1}'(t)\\end{bmatrix} = 0.$$\n\nIn other words, the gradient $\\nabla g(x)$ is perpendicular to the level sets of $g$.\n\n","cell_type":"markdown"}
{"type":"cell","id":"c0df90","pos":9,"input":"import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt","cell_type":"code","exec_count":2}
{"type":"cell","id":"e26c43","pos":26,"input":"Now, back to gradient descent.","cell_type":"markdown"}
{"type":"cell","id":"063492","pos":52,"input":"p5 = np.poly1d(np.polyfit(x, y, 5))\np5","cell_type":"code","exec_count":0}
{"type":"cell","id":"d9076b","pos":21,"input":"and 'plot' connects the dots with straight lines.","cell_type":"markdown"}
{"type":"cell","id":"4412d3","pos":27,"input":"cur_x = 6 # The algorithm starts at x=6\ngamma = 0.01 # step size multiplier\nprecision = 0.00001\nprevious_step_size = cur_x\n\nx_list = [cur_x]; y_list = [f(cur_x)]\n\ndef df(x):\n    return 4 * x**3 - 9 * x**2\n\nwhile previous_step_size > precision:\n    prev_x = cur_x\n    cur_x += -gamma * df(prev_x)\n    previous_step_size = abs(cur_x - prev_x)\n    x_list.append(cur_x)\n    y_list.append(f(cur_x))\n\nprint \"Local minimum occurs at:\", cur_x\nprint \"Number of steps:\", len(x_list)\nprint \"Minimum value:\", f(cur_x)","cell_type":"code","exec_count":0}
{"type":"cell","id":"7c3e0d","pos":44,"input":"z = np.polyfit(data[:,0],data[:,1],5)\nz","cell_type":"code","exec_count":0}
{"type":"cell","id":"804172","pos":40,"input":"# Homework\n\nSuppose that you have data consisting of points in the $x$-$y$ plane\n\n$$(x_0,y_0),(x_1,y_1),\\ldots, (x_N,y_N)$$\n\nand you'd like to find a degree $d$ polynomial $p(x)=a_0 +a_1 x +a_2 x^2 + \\cdots + a_d x^d$ that best fits the data.\n\nOne way to proceed is to think of the coefficients $a_0, \\ldots, a_d$ of the polynomial as variables and to minimize the function \n\n$$C(a_0, \\ldots, a_d)=\\sum_{i=0}^N (y_i - p(x_i))^2.$$\n\nYour problem: find the best degree $5$ polynomial that fits the data from the file datafile.npy.","cell_type":"markdown"}
{"type":"cell","id":"bb9716","pos":17,"input":"x = np.linspace(0,3,7) # make an array of 7 evenly spaced numbers between 0 and 5\nprint x","output":{"0":{"ename":"SyntaxError","evalue":"Missing parentheses in call to 'print' (<ipython-input-6-3018679ce4d0>, line 2)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-3018679ce4d0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print x\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"]}},"cell_type":"code","exec_count":6}
{"type":"cell","id":"42bb24","pos":13,"input":"def f(x):\n    return  x**4 - 3 * x**3+2","cell_type":"code","exec_count":4}
{"type":"cell","id":"224cbd","pos":23,"input":"To get a better picture, we'll use a lot more points.","cell_type":"markdown"}
{"type":"cell","id":"201486","pos":37,"input":"x0_coordlist=np.array(x_list)[:,0]\nx1_coordlist=np.array(x_list)[:,1]\nz_coordlist=np.array(z_list)","cell_type":"code","exec_count":0}
{"type":"cell","id":"8f3595","pos":51,"input":"p(0.5)","cell_type":"code","exec_count":0}
{"type":"cell","id":"54e1ae","pos":29,"input":"Let's take a closer look:","cell_type":"markdown"}
{"type":"cell","id":"92a643","pos":48,"input":"z(4)","cell_type":"code","exec_count":0}
{"type":"cell","id":"7ec9c0","pos":57,"input":"","cell_type":"code","exec_count":0}
{"type":"cell","id":"1428db","pos":46,"input":"plt.plot(x, y)","cell_type":"code","exec_count":0}
{"type":"cell","id":"42b901","pos":4,"input":"The gradient of a differentiable function $g:\\mathbb{R}^n \\to \\mathbb{R}$ is defined to be the unique vector field $\\nabla g$ with the property that for each point $x\\in \\mathbb{R}^n$, $$\\langle \\nabla g(x), v\\rangle = D_v g(x).$$\nHere, $\\langle \\, , \\, \\rangle$ denotes the inner product (dot product) of vectors and $D_v g(x)$ denotes the directional derivative of $g$ at the point $x$ in the direction $v$:\n$$D_v g(x)=\\lim_{h \\to 0} \\frac{g(x+hv)-g(x)}{h}.$$\nIn standard coordinates, the gradient can be computed as\n$$\\nabla g(x)=\\left [ \\frac{\\partial g}{\\partial x_0} (x), \\frac{\\partial g}{\\partial x_1} (x), \\ldots, \\frac{\\partial g}{\\partial x_{n-1}} (x) \\right].$$","cell_type":"markdown"}
{"type":"cell","id":"e4c2b5","pos":39,"input":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\nfig = plt.figure(figsize=(10,10))\n#ax = fig.add_subplot(1,1,1, projection='3d')\n#ax=gca(projection='3d')\n#fig = plt.figure()\nax = fig.gca (projection='3d')\nax.plot_wireframe(X0, X1, Z, rcount=10, ccount=10);\nax.plot_surface(X0, X1, Z, alpha=0.25)\nax.plot(x0_coordlist,x1_coordlist,z_coordlist,c=\"r\")\nax.view_init(20, -80)","output":{"0":{"ename":"NameError","evalue":"name 'X0' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-5a80cd1e3e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#fig = plt.figure()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_wireframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_surface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0_coordlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1_coordlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_coordlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X0' is not defined"]},"1":{"data":{"image/png":"eb96e2c25c703b42ad6a36ccc09bea8576e91a32"},"metadata":{"image/png":{"height":561,"width":575}}}},"cell_type":"code","exec_count":7}
{"type":"cell","id":"a45aa4","pos":1,"input":"# From calculation, it is expected that the local minimum occurs at x=9/4\n\ncur_x = 6 # The algorithm starts at x=6\ngamma = 0.01 # step size multiplier\nprecision = 0.00001\nprevious_step_size = cur_x\n\ndef df(x):\n    return 4 * x**3 - 9 * x**2\n\nwhile previous_step_size > precision:\n    prev_x = cur_x\n    cur_x += -gamma * df(prev_x)\n    previous_step_size = abs(cur_x - prev_x)\n\nprint(\"The local minimum occurs at %f\" % cur_x)","output":{"0":{"name":"stdout","output_type":"stream","text":"The local minimum occurs at 2.249965\n"}},"cell_type":"code","exec_count":1}
{"type":"cell","id":"678137","pos":2,"input":"To explain how gradient descent works and visualize the algorithm geometrically, it will help to recall a bit about gradients.","cell_type":"markdown"}
{"type":"cell","id":"7f9635","pos":54,"input":"x","cell_type":"code","exec_count":0}
{"type":"cell","id":"204363","pos":42,"input":"data","cell_type":"code","exec_count":0}
{"type":"cell","id":"bcbaf2","pos":3,"input":"## A quick review about gradients and two important facts","cell_type":"markdown"}
{"type":"cell","id":"fae83b","pos":53,"input":"xp = np.linspace(-2, 6, 100)\n_ = plt.plot(x, y, '.', xp, p(xp), '-', xp, p5(xp), '--')\nplt.show()","cell_type":"code","exec_count":0}
{"type":"cell","id":"1a0954","pos":24,"input":"x = np.linspace(0,6,1001)\ny = f(x)","cell_type":"code","exec_count":0}
{"type":"cell","id":"8cd096","pos":22,"input":"fig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\naxes.plot(x,y, label=r'$y = x^2$')\naxes.legend(loc=2); # upper left corner;","cell_type":"code","exec_count":0}
{"type":"cell","id":"04579e","pos":32,"input":"Here we consider a function $g:\\mathbb{R}^2 \\to \\mathbb{R}$ defined by $g(x_0,x_1)=3(x_0-2)^2+(x_1-1)^2$.  We define $g$ as a function of $x$, where $x$ should be a list $[x_0,x_1].$","cell_type":"markdown"}
{"type":"cell","id":"685867","pos":50,"input":"p = np.poly1d(z)","cell_type":"code","exec_count":0}
{"type":"cell","id":"ebd323","pos":14,"input":"Here's how to evaluate this function at a number, say $2$.","cell_type":"markdown"}
{"type":"cell","id":"d842d2","pos":35,"input":"x_old = np.array([0,4])\nh = 0.1 # step size\nprecision = 0.001\n\nx_list = [x_old]\nz_list = [g(x_old)]\n\nx_new = x_old - h * grad_g(x_old)\nx_list.append(x_new)\nz_list.append(g(x_new))\n\nwhile (abs(x_new[0] - x_old[0])+abs(x_new[1] - x_old[1])) > precision:\n    x_old = x_new\n    direction = - grad_g(x_old)\n    x_new = x_old + h * direction\n    x_list.append(x_new)\n    z_list.append(g(x_new))\nprint \"Local minimum occurs at:\", x_new\nprint \"Number of steps:\", len(x_list)\nprint \"minimum value is:\", g(x_new)","cell_type":"code","exec_count":0}
{"type":"cell","id":"97a27e","pos":43,"input":"x=data[:,0]\nx","cell_type":"code","exec_count":0}
{"type":"cell","id":"aae895","pos":10,"input":"The following line is a Jupyter code that tells matplotlib to display graphics inline within the jupyter notbook","cell_type":"markdown"}
{"type":"cell","id":"9a67d2","pos":8,"input":"## Import modules","cell_type":"markdown"}
{"type":"cell","id":"2b3f60","pos":18,"input":"y=x**2 # squares every number in the list\nprint(y)","cell_type":"code","exec_count":0}
{"type":"cell","id":"69e26e","pos":49,"input":"z","cell_type":"code","exec_count":0}
{"type":"cell","id":"dd4c6d","pos":45,"input":"y = data[:,1]\ny","cell_type":"code","exec_count":0}
{"type":"cell","id":"60e58f","pos":41,"input":"# load the data:\ndata=np.load('datafile.npy')\nfig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7]) \naxes.scatter(data[:,0],data[:,1],c=\"r\");","cell_type":"code","exec_count":0}
{"type":"cell","id":"e1013a","pos":20,"input":"fig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\naxes.scatter(x,y);","cell_type":"code","exec_count":0}
{"type":"cell","id":"f33484","pos":55,"input":"y","cell_type":"code","exec_count":0}
{"type":"cell","id":"c8dc10","pos":25,"input":"fig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\naxes.plot(x,y, label=r'$y = x^4 - 3  x^3+2$')\naxes.legend(loc=2); # upper left corner;","cell_type":"code","exec_count":0}
{"type":"cell","id":"0cce22","pos":7,"input":"Now, let's do some programming.","cell_type":"markdown"}
{"type":"cell","id":"6b5e07","pos":30,"input":"xzoom = np.linspace(0,2.5,100)\nyzoom=f(xzoom)\n\nmatplotlib.rcParams.update({'font.size': 12, 'text.usetex': True})\n\nfig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7]) # left, bottom, width, height (range 0 to 1)\naxes.plot(xzoom,yzoom, 'g', label=r'$y = x^4-3x^3+2$') # g for green\naxes.scatter(x_list[8:],y_list[8:],c=\"r\")\naxes.plot(x_list[8:],y_list[8:],c=\"r\",label='gradient descent steps')\naxes.legend(loc=1); # upper right corner\naxes.set_xlabel('$x$')\naxes.set_ylabel('$y$') ;","cell_type":"code","exec_count":0}
{"type":"cell","id":"a56916","pos":36,"input":"x0 = np.linspace(-1,6,100)\nx1 = np.linspace(-2,5,100)\nX0, X1 = np.meshgrid(x0,x1)\nZ=g([X0,X1])","cell_type":"code","exec_count":0}
{"type":"cell","id":"391136","pos":11,"input":"%matplotlib inline","cell_type":"code","exec_count":3}
{"type":"cell","id":"834796","pos":0,"input":"# Basic Gradient Descent\n\nGradient descent is an important algorithm for minimizing a function.  Having an algorithm to minimize a function is quite powerful:  you can maximize a function $f$ by minimizing $-f$ and you can solve a system of equations $f_1=k_1, f_2=k_2, \\cdots f_r=k_r$ by minimizing $(f_1-k_1)^2+(f_2-k_2)^2+\\cdots (f_r-k_r)^2$.  \n\nHere's an example, in one variable, implemented in python, from <a href=\"https://en.wikipedia.org/wiki/Gradient_descent#Computational_examples\">https://en.wikipedia.org/wiki/Gradient_descent#Computational_examples</a>.  The program finds the minimum of\n$ f(x) =x^4âˆ’3x^3+2. $","cell_type":"markdown"}
{"type":"cell","id":"55c2e3","pos":12,"input":"Define a function","cell_type":"markdown"}
{"type":"cell","id":"83f24c","pos":34,"input":"def grad_g(x):\n    return np.array([6*(x[0]-2), 2.0*(x[1]-1)])","cell_type":"code","exec_count":0}
{"type":"cell","id":"344acd","pos":47,"input":"xp = np.linspace(-2, 6, 100)\n>>> _ = plt.plot(x, y, '.', xp, p(xp), '-', xp, p30(xp), '--')","cell_type":"code","exec_count":0}
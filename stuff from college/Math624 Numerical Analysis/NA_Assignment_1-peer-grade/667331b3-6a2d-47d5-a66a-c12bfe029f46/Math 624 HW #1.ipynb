{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# HW#1 for Math 624: Numerical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First we import our libraries and the data we're going to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data=np.load('datafile.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is my general gradient descent function. \"ip\" is a numpy array, \"h\" is a scalar step size, \"tol 1\" is the scalar tolerance and \"m\" is a natural denoting the maximum number of allowed iterations. This is analogous to the one I will use to fit the 5th degree polynomial on \"data\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def GD(ip, h, tol1, m):\n",
    "    x_old = ip\n",
    "    num_iter = 0\n",
    "    \n",
    "    x_list = [x_old]\n",
    "    z_list = [g(x_old)]\n",
    "    \n",
    "    x_new = x_old - grad_g(x_old)\n",
    "    x_list.append(x_new)\n",
    "    z_list.append(g(x_new))\n",
    "    \n",
    "    while num_iter < m and (abs(x_new[0] - x_old[0]) + abs(x_new[1] - x_old[1])) > tol1:\n",
    "        x_old = x_new\n",
    "        direction = -1*h*grad_g(x_old)\n",
    "        x_new = x_old + direction\n",
    "        x_list.append(x_new)\n",
    "        z_list.append(g(x_new))\n",
    "        num_iter += 1\n",
    "        \n",
    "    print (\"Local minimum occurs at:\", x_new)\n",
    "    print (\"Number of steps:\", num_iter)\n",
    "    print (\"minimum value is:\", g(x_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This function finds the value of a \"dth\" degree polynomial at \"x\" given a list of coefficients \"a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def p(a,x,d):\n",
    "    val = 0\n",
    "    for i in range(0,d+1):\n",
    "        val += a[i]*x**i\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is our cost function. \"a\" will consistently be our parameter that is a np array of changing coefficients just as \"data_points\" will consistently be our parameter containing the data points to sum over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def g(a, data_points):\n",
    "    sum = 0\n",
    "    for point in data_points:\n",
    "        sum += (p(a,point[0],5) - point[1])**2\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is our gradient function. It returns a vector of size d+1 and invokes the chain rule. Note that in the loss function, $$(p(x_i)-y_i)^2 = (y_i - p(x_i))^2 $$ so if you want to use the chain rule and avoid multiplying by -1, you can just switch the order like above. We don't need the 2 factor outside our sum since wherever the gradient is very close to 0, you can divide 2 and the gradient will still be very close to 0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def grad_g(a, data_points):\n",
    "    gradient = np.array([0,0,0,0,0,0])\n",
    "    \n",
    "    for i in range(len(gradient)):\n",
    "        sum = 0\n",
    "        \n",
    "        for point in data_points:\n",
    "            sum += (p(a,point[0],5) - point[1]) * point[0]**i\n",
    "            \n",
    "        gradient[i] = sum\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# The norm of a general vector in R^n\n",
    "def norm(arr):\n",
    "    n = 0\n",
    "    for el in arr:\n",
    "        n += el**2     \n",
    "    return n**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def GDnew(ip, h, tol1, m, data_points):\n",
    "    # Our initial vector and the number of iterations declared and assigned\n",
    "    x_old = ip\n",
    "    num_iter = 0\n",
    "    \n",
    "    # Our list of x and \"g\" values that will log all the passing values of x and g(x)\n",
    "    x_list = [x_old]\n",
    "    g_list = [g(x_old, data_points)]\n",
    "    \n",
    "    # Our second vector x_1 which is computed with the norm of the gradient\n",
    "    # and then added to the list of x vectors\n",
    "    x_new = x_old - grad_g(x_old, data_points)/norm(grad_g(x_old, data_points))\n",
    "    x_list.append(x_new)\n",
    "    g_list.append(g(x_new, data_points))\n",
    "    \n",
    "    # Loops until we hit our max iterations or the sum of errors are below our tolerance\n",
    "    while num_iter < m and (abs(x_new[0] - x_old[0]) + abs(x_new[1] - x_old[1]) + abs(x_new[2] - x_old[2]) + abs(x_new[3] - x_old[3]) + abs(x_new[4] - x_old[4]) + abs(x_new[5] - x_old[5])) > tol1:\n",
    "        x_old = x_new\n",
    "        direction = -1*h*grad_g(x_old, data_points)/norm(grad_g(x_old, data_points))*1.0\n",
    "        x_new = x_old + direction\n",
    "        x_list.append(x_new)\n",
    "        g_list.append(g(x_new, data_points))\n",
    "        num_iter += 1\n",
    "        \n",
    "    print (\"Local minimum occurs at:\", x_new)\n",
    "    print (\"Number of steps:\", num_iter)\n",
    "    print (\"minimum value is:\", g(x_new, data_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The \"arr\" list serves as our initial point and \"arr2\" serves as our object containing the weights of our polynomial. Increasing the maximum number of iterations and decreasing the step size will make the minimum value go smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local minimum occurs at: [ 0.76968247  0.71407981  0.86516394  0.63230299 -0.98290423  0.20647847]\n",
      "Number of steps: 50000\n",
      "minimum value is: 2502.08677072\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([0,0,0,0,0,0])\n",
    "arr2 = GDnew(arr, 0.005, 0.001, 50000, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This just plots our estimated function (blue) against the data points (red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def f(x):\n",
    "    return 0.76968247 + 0.71407981*x + 0.86516394*x**2 + 0.63230299*x**3 + -0.98290423*x**4 + 0.20647847*x**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fee1b7da7f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.1, 0.1, 0.9, 0.7]) \n",
    "axes.scatter(data[:,0],data[:,1],c=\"r\");\n",
    "\n",
    "new_x = np.linspace(0, 5, 150)\n",
    "new_y = f(new_x)\n",
    "\n",
    "axes.scatter(new_x,new_y,c=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
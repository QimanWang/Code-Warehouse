{"exec_count":9,"start":1506441146665,"input":"def f(x):\n    return 0.76968247 + 0.71407981*x + 0.86516394*x**2 + 0.63230299*x**3 + -0.98290423*x**4 + 0.20647847*x**5","state":"done","pos":16,"cell_type":"code","type":"cell","end":1506441146699,"id":"aef9af","kernel":"python3"}
{"exec_count":5,"start":1506441054497,"input":"def grad_g(a, data_points):\n    gradient = np.array([0,0,0,0,0,0])\n    \n    for i in range(len(gradient)):\n        sum = 0\n        \n        for point in data_points:\n            sum += (p(a,point[0],5) - point[1]) * point[0]**i\n            \n        gradient[i] = sum\n    \n    return gradient","state":"done","pos":10,"cell_type":"code","type":"cell","end":1506441054510,"id":"4ce434","kernel":"python3"}
{"type":"cell","id":"02c0c1","pos":3,"input":"This is my general gradient descent function. \"ip\" is a numpy array, \"h\" is a scalar step size, \"tol 1\" is the scalar tolerance and \"m\" is a natural denoting the maximum number of allowed iterations. This is analogous to the one I will use to fit the 5th degree polynomial on \"data\". ","cell_type":"markdown"}
{"exec_count":2,"start":1506441050399,"input":"def GD(ip, h, tol1, m):\n    x_old = ip\n    num_iter = 0\n    \n    x_list = [x_old]\n    z_list = [g(x_old)]\n    \n    x_new = x_old - grad_g(x_old)\n    x_list.append(x_new)\n    z_list.append(g(x_new))\n    \n    while num_iter < m and (abs(x_new[0] - x_old[0]) + abs(x_new[1] - x_old[1])) > tol1:\n        x_old = x_new\n        direction = -1*h*grad_g(x_old)\n        x_new = x_old + direction\n        x_list.append(x_new)\n        z_list.append(g(x_new))\n        num_iter += 1\n        \n    print (\"Local minimum occurs at:\", x_new)\n    print (\"Number of steps:\", num_iter)\n    print (\"minimum value is:\", g(x_new))","state":"done","pos":4,"cell_type":"code","type":"cell","end":1506441050419,"id":"258972","kernel":"python3"}
{"type":"cell","id":"5dec3e","pos":18,"input":"","cell_type":"code"}
{"exec_count":7,"start":1506441057305,"input":"def GDnew(ip, h, tol1, m, data_points):\n    # Our initial vector and the number of iterations declared and assigned\n    x_old = ip\n    num_iter = 0\n    \n    # Our list of x and \"g\" values that will log all the passing values of x and g(x)\n    x_list = [x_old]\n    g_list = [g(x_old, data_points)]\n    \n    # Our second vector x_1 which is computed with the norm of the gradient\n    # and then added to the list of x vectors\n    x_new = x_old - grad_g(x_old, data_points)/norm(grad_g(x_old, data_points))\n    x_list.append(x_new)\n    g_list.append(g(x_new, data_points))\n    \n    # Loops until we hit our max iterations or the sum of errors are below our tolerance\n    while num_iter < m and (abs(x_new[0] - x_old[0]) + abs(x_new[1] - x_old[1]) + abs(x_new[2] - x_old[2]) + abs(x_new[3] - x_old[3]) + abs(x_new[4] - x_old[4]) + abs(x_new[5] - x_old[5])) > tol1:\n        x_old = x_new\n        direction = -1*h*grad_g(x_old, data_points)/norm(grad_g(x_old, data_points))*1.0\n        x_new = x_old + direction\n        x_list.append(x_new)\n        g_list.append(g(x_new, data_points))\n        num_iter += 1\n        \n    print (\"Local minimum occurs at:\", x_new)\n    print (\"Number of steps:\", num_iter)\n    print (\"minimum value is:\", g(x_new, data_points))","state":"done","pos":12,"cell_type":"code","type":"cell","end":1506441057334,"id":"f8fe73","kernel":"python3"}
{"exec_count":4,"start":1506441053016,"input":"def g(a, data_points):\n    sum = 0\n    for point in data_points:\n        sum += (p(a,point[0],5) - point[1])**2\n    return sum","state":"done","pos":8,"cell_type":"code","type":"cell","end":1506441053108,"id":"548931","kernel":"python3"}
{"type":"cell","id":"9d58a6","pos":9,"input":"This is our gradient function. It returns a vector of size d+1 and invokes the chain rule. Note that in the loss function, $$(p(x_i)-y_i)^2 = (y_i - p(x_i))^2 $$ so if you want to use the chain rule and avoid multiplying by -1, you can just switch the order like above. We don't need the 2 factor outside our sum since wherever the gradient is very close to 0, you can divide 2 and the gradient will still be very close to 0. \n","cell_type":"markdown"}
{"output":{"0":{"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7fee1b7da7f0>"},"exec_count":10},"1":{"data":{"image/png":"30520d28620d6e99043fd84518cb28ee573db588"}}},"exec_count":10,"start":1506441146703,"input":"fig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7]) \naxes.scatter(data[:,0],data[:,1],c=\"r\");\n\nnew_x = np.linspace(0, 5, 150)\nnew_y = f(new_x)\n\naxes.scatter(new_x,new_y,c=\"b\")","state":"done","pos":17,"cell_type":"code","type":"cell","end":1506441147321,"id":"b91037","kernel":"python3"}
{"type":"cell","id":"72ee3f","pos":13,"input":"The \"arr\" list serves as our initial point and \"arr2\" serves as our object containing the weights of our polynomial. Increasing the maximum number of iterations and decreasing the step size will make the minimum value go smaller.","cell_type":"markdown"}
{"exec_count":3,"start":1506441051700,"input":"def p(a,x,d):\n    val = 0\n    for i in range(0,d+1):\n        val += a[i]*x**i\n    return val","state":"done","pos":6,"cell_type":"code","type":"cell","end":1506441051706,"id":"bfc3c7","kernel":"python3"}
{"output":{"0":{"name":"stdout","text":"Local minimum occurs at: [ 0.76968247  0.71407981  0.86516394  0.63230299 -0.98290423  0.20647847]\nNumber of steps: 50000\nminimum value is: 2502.08677072\n"}},"exec_count":8,"start":1506441058699,"input":"arr = np.array([0,0,0,0,0,0])\narr2 = GDnew(arr, 0.005, 0.001, 50000, data)","state":"done","pos":14,"cell_type":"code","type":"cell","end":1506441146661,"id":"a9753a","kernel":"python3"}
{"type":"settings","kernel":"python3","backend_state":"running","metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"kernel_state":"idle","trust":true}
{"type":"file","last_load":1505612749177}
{"type":"cell","id":"f3a281","pos":5,"input":"This function finds the value of a \"dth\" degree polynomial at \"x\" given a list of coefficients \"a\".","cell_type":"markdown"}
{"type":"cell","id":"a15201","pos":0,"input":"# HW#1 for Math 624: Numerical Analysis","cell_type":"markdown"}
{"exec_count":1,"start":1506441049094,"input":"import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndata=np.load('datafile.npy')","state":"done","pos":2,"cell_type":"code","type":"cell","end":1506441049121,"id":"c7caba","kernel":"python3"}
{"type":"cell","id":"f15271","pos":7,"input":"This is our cost function. \"a\" will consistently be our parameter that is a np array of changing coefficients just as \"data_points\" will consistently be our parameter containing the data points to sum over.","cell_type":"markdown","collapsed":true}
{"exec_count":6,"start":1506441055797,"input":"# The norm of a general vector in R^n\ndef norm(arr):\n    n = 0\n    for el in arr:\n        n += el**2     \n    return n**0.5","state":"done","pos":11,"cell_type":"code","type":"cell","end":1506441055802,"id":"e44193","kernel":"python3"}
{"type":"cell","id":"5f0920","pos":15,"input":"This just plots our estimated function (blue) against the data points (red).","cell_type":"markdown"}
{"type":"cell","id":"9ffaa2","pos":1,"input":"First we import our libraries and the data we're going to fit.","cell_type":"markdown"}
{"type":"cell","id":"f29826","pos":19,"input":""}
{"type":"settings","kernel":"python2","backend_state":"running","trust":true,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"}},"kernel_state":"idle"}
{"type":"cell","id":"824867","pos":6,"input":"### Perpendicular to level curves\n\nThe level set of $g:\\mathbb{R}^n \\to \\mathbb{R}$ of level $k\\in \\mathbb{R}$ is defined to be the set\n\n$$g^{-1}(k)=\\{(x_0,x_1,  \\ldots, x_{n-1})\\in \\mathbb{R}^n: g(x_0,x_1,  \\ldots, x_{n-1})=k\\}$$\n\nIf $:\\alpha: t\\mapsto (x_0(t),x_1(t),  \\ldots, x_{n-1}(t))$ is a curve in a level set of $g$, then the the function $\\mathbb{R} \\overset{\\alpha}{\\to} \\mathbb{R}^n \\overset{g}{\\to} \\mathbb{R}$ is constant and hence has zero derivative.  Keeping the multivariable chain rule in mind, we have\n\n$$Dg \\circ d\\alpha = 0 \\Longleftrightarrow \\begin{bmatrix} \\frac{\\partial g}{\\partial x_0} (x)& \\frac{\\partial g}{\\partial x_1} (x)& \\ldots & \\frac{\\partial g}{\\partial x_{n-1}} (x) \\end{bmatrix} \\begin{bmatrix} x_0'(t) \\\\ x_1'(t) \\\\ \\vdots \\\\ x_{n-1}'(t)\\end{bmatrix} = 0.$$\n\nIn other words, the gradient $\\nabla g(x)$ is perpendicular to the level sets of $g$.\n\n","cell_type":"markdown"}
{"type":"cell","id":"739c30","pos":0,"input":"# Basic Gradient Descent\n\nGradient descent is an important algorithm for minimizing a function.  Having an algorithm to minimize a function is quite powerful:  you can maximize a function $f$ by minimizing $-f$ and you can solve a system of equations $f_1=k_1, f_2=k_2, \\cdots f_r=k_r$ by minimizing $(f_1-k_1)^2+(f_2-k_2)^2+\\cdots (f_r-k_r)^2$.  \n\nHere's an example, in one variable, implemented in python, from <a href=\"https://en.wikipedia.org/wiki/Gradient_descent#Computational_examples\">https://en.wikipedia.org/wiki/Gradient_descent#Computational_examples</a>.  The program finds the minimum of\n$ f(x) =x^4âˆ’3x^3+2. $","cell_type":"markdown"}
{"type":"cell","id":"0dce22","pos":1,"input":"# From calculation, it is expected that the local minimum occurs at x=9/4\n\ncur_x = 6 # The algorithm starts at x=6\ngamma = 0.01 # step size multiplier\nprecision = 0.00001\nprevious_step_size = cur_x\n\ndef df(x):\n    return 4 * x**3 - 9 * x**2\n\nwhile previous_step_size > precision:\n    prev_x = cur_x\n    cur_x += -gamma * df(prev_x)\n    previous_step_size = abs(cur_x - prev_x)\n\nprint(\"The local minimum occurs at %f\" % cur_x)","cell_type":"code","exec_count":0}
{"type":"cell","id":"ca839c","pos":2,"input":"To explain how gradient descent works and visualize the algorithm geometrically, it will help to recall a bit about gradients.","cell_type":"markdown"}
{"type":"cell","id":"7eaaca","pos":3,"input":"## A quick review about gradients and two important facts","cell_type":"markdown"}
{"type":"cell","id":"9e5204","pos":4,"input":"The gradient of a differentiable function $g:\\mathbb{R}^n \\to \\mathbb{R}$ is defined to be the unique vector field $\\nabla g$ with the property that for each point $x\\in \\mathbb{R}^n$, $$\\langle \\nabla g(x), v\\rangle = D_v g(x).$$\nHere, $\\langle \\, , \\, \\rangle$ denotes the inner product (dot product) of vectors and $D_v g(x)$ denotes the directional derivative of $g$ at the point $x$ in the direction $v$:\n$$D_v g(x)=\\lim_{h \\to 0} \\frac{g(x+hv)-g(x)}{h}.$$\nIn standard coordinates, the gradient can be computed as\n$$\\nabla g(x)=\\left [ \\frac{\\partial g}{\\partial x_0} (x), \\frac{\\partial g}{\\partial x_1} (x), \\ldots, \\frac{\\partial g}{\\partial x_{n-1}} (x) \\right].$$","cell_type":"markdown"}
{"type":"cell","id":"b96321","pos":5,"input":"### Steepest Descent\n\nRecall that $\\langle v,w \\rangle=\\|v\\|w\\| \\cos(\\theta)$ where $\\theta$ is the angle between the vectors $v$ and $w$.  Therefore, if $v$ is a unit vector, \n\n$$D_v g(x)=\\langle \\nabla g(x), v\\rangle =\\|\\nabla g(x)\\| \\cos(\\theta)$$\n\nis maximized when $v$ points in the direction of the gradient $\\nabla g(x)$ and minimized when $v$ points in the direction $-\\nabla g(x)$.\n\n<strong>Conclusion</strong> $-\\nabla g(x)$ is the direction of steepest descent for the function $g$ at the point $x$.","cell_type":"markdown"}
{"type":"cell","id":"4d1858","pos":7,"input":"Now, let's do some programming.","cell_type":"markdown"}
{"type":"cell","id":"d02046","pos":8,"input":"## Import modules","cell_type":"markdown"}
{"type":"cell","id":"9ba015","pos":9,"input":"import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt","cell_type":"code","exec_count":0}
{"type":"cell","id":"15199f","pos":10,"input":"The following line is a Jupyter code that tells matplotlib to display graphics inline within the jupyter notbook","cell_type":"markdown"}
{"type":"cell","id":"20b92b","pos":11,"input":"%matplotlib inline","cell_type":"code","exec_count":0}
{"type":"cell","id":"5c179f","pos":12,"input":"Define a function","cell_type":"markdown"}
{"type":"cell","id":"f0dd58","pos":13,"input":"def f(x):\n    return  x**4 - 3 * x**3+2","cell_type":"code","exec_count":0,"collapsed":true}
{"type":"cell","id":"8bf43f","pos":14,"input":"Here's how to evaluate this function at a number, say $2$.","cell_type":"markdown"}
{"type":"cell","id":"da581d","pos":15,"input":"f(2)","cell_type":"code","exec_count":0}
{"type":"cell","id":"8f57fc","pos":16,"input":"To plot this function, we make a bunch of (x,y) coordinates on the graph.  Before doing this, let's take a look at ways we can operate on elements in a list.","cell_type":"markdown"}
{"type":"cell","id":"a23c91","pos":17,"input":"x = np.linspace(0,3,7) # make an array of 7 evenly spaced numbers between 0 and 5\nprint x","cell_type":"code","exec_count":0}
{"type":"cell","id":"f0c9d6","pos":18,"input":"y=x**2 # squares every number in the list\nprint(y)","cell_type":"code","exec_count":0}
{"type":"cell","id":"1ba0f4","pos":19,"input":"'scatter' plots the points as dots","cell_type":"markdown"}
{"type":"cell","id":"9fd1dc","pos":20,"input":"fig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\naxes.scatter(x,y);","cell_type":"code","exec_count":0}
{"type":"cell","id":"537bfd","pos":21,"input":"and 'plot' connects the dots with straight lines.","cell_type":"markdown"}
{"type":"cell","id":"0b534e","pos":22,"input":"fig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\naxes.plot(x,y, label=r'$y = x^2$')\naxes.legend(loc=2); # upper left corner;","cell_type":"code","exec_count":0}
{"type":"cell","id":"97a71b","pos":23,"input":"To get a better picture, we'll use a lot more points.","cell_type":"markdown"}
{"type":"cell","id":"9f049c","pos":24,"input":"x = np.linspace(0,6,1001)\ny = f(x)","cell_type":"code","exec_count":0,"collapsed":true}
{"type":"cell","id":"c979ef","pos":25,"input":"fig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\naxes.plot(x,y, label=r'$y = x^4 - 3  x^3+2$')\naxes.legend(loc=2); # upper left corner;","cell_type":"code","exec_count":0}
{"type":"cell","id":"f1b01f","pos":26,"input":"Now, back to gradient descent.","cell_type":"markdown"}
{"type":"cell","id":"d29ea4","pos":27,"input":"cur_x = 6 # The algorithm starts at x=6\ngamma = 0.01 # step size multiplier\nprecision = 0.00001\nprevious_step_size = cur_x\n\nx_list = [cur_x]; y_list = [f(cur_x)]\n\ndef df(x):\n    return 4 * x**3 - 9 * x**2\n\nwhile previous_step_size > precision:\n    prev_x = cur_x\n    cur_x += -gamma * df(prev_x)\n    previous_step_size = abs(cur_x - prev_x)\n    x_list.append(cur_x)\n    y_list.append(f(cur_x))\n\nprint \"Local minimum occurs at:\", cur_x\nprint \"Number of steps:\", len(x_list)\nprint \"Minimum value:\", f(cur_x)","cell_type":"code","exec_count":0}
{"type":"cell","id":"5e439d","pos":28,"input":"fig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7]) # left, bottom, width, height (range 0 to 1)\naxes.plot(x,y, 'g', label=r'$y = x^4-3x^3+2$') # g for green\naxes.scatter(x_list,y_list,c=\"r\")\naxes.plot(x_list,y_list,c=\"r\",label='gradient descent steps')\naxes.legend(loc=2); # upper left corner\naxes.set_xlabel('$x$')\naxes.set_ylabel('$y$') ;","cell_type":"code","exec_count":0}
{"type":"cell","id":"04b0f5","pos":29,"input":"Let's take a closer look:","cell_type":"markdown"}
{"type":"cell","id":"779d8d","pos":30,"input":"xzoom = np.linspace(0,2.5,100)\nyzoom=f(xzoom)\n\nmatplotlib.rcParams.update({'font.size': 12, 'text.usetex': True})\n\nfig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7]) # left, bottom, width, height (range 0 to 1)\naxes.plot(xzoom,yzoom, 'g', label=r'$y = x^4-3x^3+2$') # g for green\naxes.scatter(x_list[8:],y_list[8:],c=\"r\")\naxes.plot(x_list[8:],y_list[8:],c=\"r\",label='gradient descent steps')\naxes.legend(loc=1); # upper right corner\naxes.set_xlabel('$x$')\naxes.set_ylabel('$y$') ;","cell_type":"code","exec_count":0}
{"type":"cell","id":"ab1df2","pos":31,"input":"## 2 dimensional example","cell_type":"markdown"}
{"type":"cell","id":"eeb4e0","pos":32,"input":"Here we consider a function $g:\\mathbb{R}^2 \\to \\mathbb{R}$ defined by $g(x_0,x_1)=3(x_0-2)^2+(x_1-1)^2$.  We define $g$ as a function of $x$, where $x$ should be a list $[x_0,x_1].$","cell_type":"markdown"}
{"type":"cell","id":"a070df","pos":33,"input":"def g(x):\n    return 3*(x[0]-2)**2+(x[1]-1)**2+5","cell_type":"code","exec_count":0,"collapsed":true}
{"type":"cell","id":"1cf71f","pos":34,"input":"def grad_g(x):\n    return np.array([6*(x[0]-2), 2.0*(x[1]-1)])","cell_type":"code","exec_count":0,"collapsed":true}
{"type":"cell","id":"3036eb","pos":35,"input":"x_old = np.array([0,4])\nh = 0.1 # step size\nprecision = 0.001\n\nx_list = [x_old]\nz_list = [g(x_old)]\n\nx_new = x_old - h * grad_g(x_old)\nx_list.append(x_new)\nz_list.append(g(x_new))\n\nwhile (abs(x_new[0] - x_old[0])+abs(x_new[1] - x_old[1])) > precision:\n    x_old = x_new\n    direction = - grad_g(x_old)\n    x_new = x_old + h * direction\n    x_list.append(x_new)\n    z_list.append(g(x_new))\nprint \"Local minimum occurs at:\", x_new\nprint \"Number of steps:\", len(x_list)\nprint \"minimum value is:\", g(x_new)","cell_type":"code","exec_count":0}
{"type":"cell","id":"0eaaf2","pos":36,"input":"x0 = np.linspace(-1,6,100)\nx1 = np.linspace(-2,5,100)\nX0, X1 = np.meshgrid(x0,x1)\nZ=g([X0,X1])","cell_type":"code","exec_count":0,"collapsed":true}
{"type":"cell","id":"35d87e","pos":37,"input":"x0_coordlist=np.array(x_list)[:,0]\nx1_coordlist=np.array(x_list)[:,1]\nz_coordlist=np.array(z_list)","cell_type":"code","exec_count":0}
{"type":"cell","id":"9c1034","pos":38,"input":"plt.figure()\nplt.contour(X0,X1,Z,50) # this plots level sets\nplt.plot(x0_coordlist,x1_coordlist,c=\"r\"); # here is the gradient path","cell_type":"code","exec_count":0}
{"type":"cell","id":"22d2f0","pos":39,"input":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\nfig = plt.figure(figsize=(10,10))\n#ax = fig.add_subplot(1,1,1, projection='3d')\n#ax=gca(projection='3d')\n#fig = plt.figure()\nax = fig.gca (projection='3d')\nax.plot_wireframe(X0, X1, Z, rcount=10, ccount=10);\nax.plot_surface(X0, X1, Z, alpha=0.25)\nax.plot(x0_coordlist,x1_coordlist,z_coordlist,c=\"r\")\nax.view_init(20, -80)","cell_type":"code","exec_count":0}
{"type":"cell","id":"79ad99","pos":40,"input":"# Homework\n\nSuppose that you have data consisting of points in the $x$-$y$ plane\n\n$$(x_0,y_0),(x_1,y_1),\\ldots, (x_N,y_N)$$\n\nand you'd like to find a degree $d$ polynomial $p(x)=a_0 +a_1 x +a_2 x^2 + \\cdots + a_d x^d$ that best fits the data.\n\nOne way to proceed is to think of the coefficients $a_0, \\ldots, a_d$ of the polynomial as variables and to minimize the function \n\n$$C(a_0, \\ldots, a_d)=\\sum_{i=0}^N (y_i - p(x_i))^2.$$\n\nYour problem: find the best degree $5$ polynomial that fits the data from the file datafile.npy.","cell_type":"markdown"}
{"type":"cell","id":"d0c8c7","pos":41,"input":"# load the data:\ndata=np.load('datafile.npy')\nfig = plt.figure()\naxes = fig.add_axes([0.1, 0.1, 0.9, 0.7]) \naxes.scatter(data[:,0],data[:,1],c=\"r\");","output":{"0":{"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cbe20bfe3d4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datafile.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}},"cell_type":"code","exec_count":1}
{"type":"cell","id":"6305ab","pos":42,"input":"","cell_type":"code","exec_count":0,"collapsed":true}
{"type":"file","last_load":1506441109103}